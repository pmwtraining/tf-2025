# Module 3: Working with complex variables and Terraform modules

# The Challenge

In the previous module you have hardened your virtual machine and made your template more reusable. Terraform is all about templates and reusability. For almost all imagineable cloud resources, and even whole reference architectures, there are templates (or **Modules** in Terraform) available in the [Terraform registry](https://registry.terraform.io/browse/modules).

In this module, we will continue working on our previous work. If you have not completed the previous module, you can use the full code example in the [full_solution folder in module-2](../module-2/full_solution).
The ultimate goal is to turn your Terraform code into a module and deploy it for re-use. You will also work with remote state backends, so you are fully ready to start deploying your work in the cloud!

> Assignments are marked like this.

<details>
<summary>Solutions are shown like this.</summary>
    Hi! Only open these when you are completely clueless.
</details>
<p></p>

And you can also earn **Bonus points**! These are not included in the full solution, so it's fully up to you on how you solve these challenges!

See how far you can get during the workshop:

- **Level 1: Use functions and expressions to dynamically generate resources/properties**
- **Level 2: Work with modules in your Terraform configuration**
- **Level 3: Store state in a remote backend**
- **Level 4: Turn your code into a reusable module**
- **Level 5: Get certified!**

Some general tips before you start:

- With Terraform, you basically create a template to deploy infrastructure. This means that a lot of examples are available all over the internet. You can often find a module or template that allows you to only fill out some variables in order to deploy a resource that matches your needs.
- Think of Terraform as an API layer for the Azure API itself. Any option you see in the Azure portal, or remember from your work, has an equivalent property in the corresponding resource. So if you are lost, go to the portal and try creating a resource manually. The properties you see there reflect the properties you can set for that resource in Terraform. Just use the [provider documentation](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs) to find out the name of the property for that resource.
- Always run `terraform plan` before you run `terraform apply`. Terraform will clearly show you the impact your code changes will have on the real world infrastructure. You can experiment all you want if you use `plan`.
- Check out [this](https://www.tfwriter.com/azurerm/azurerm.html) awesome website to grab some autogenerated Terraform code. If you are ever in need of some prefabricated code for creating a resource, variable or output you can get it here. This one is for the advanced users, but once you find out how useful it is you keep coming back!
- If you run into any issues anywhere, just run `terraform destroy` and `terraform apply` again. The beauty of declarative languages is you get exactly the same infrastructure back the way you wrote it in your code :)
  
## Level 1: Use functions and expressions to dynamically generate resources/properties

While Terraform is not technically a programming language, there are several built-in functions you can use to generate dynamically generate properties or resources. These functions allow string manipulation, collections, logical functions and much more. Read more about them [here](https://www.terraform.io/language/functions). You have already worked with a few functions in the previous modules, such as the `regex` function to validate variable values. We will work with some common functions in the upcoming excercises.

> Let's start simple. Use a string manipulation feature to remove the dashes ('-') from `local.rootname` to generate `local.trimmed_rootname`.

<details>
<summary>Solution</summary>

```hcl
# main.tf
locals {
  rootname         = "bctf-${var.yourname}-${var.location}"
  trimmed_rootname = replace(local.rootname, "-", "")
...
```

</details>

Run `terraform plan` and verify this changes nothing to your configuration, but it makes your code a little bit nicer.<p>

We are currently defining our tags in the `local.tags` block so they can be the same for all resources we deploy. But what if we want to allow users of our template to specify additional custom tags, or we want to have additional tags for specific resources?

> Create a variable definition called `additional_tags` (use the correct type!) and ensure that it is added to all resources we create using the same `locals.tags` block. Add a custom `key=value` tag to your `.tfvars` file. Run `terraform plan` to ensure all your resources are being **changed**.

<details>
<summary>Solution</summary>

```hcl
# variables.tf
variable "additional_tags" {
  description = "A list of additional tags that can be added to the centralized tags block"

  type    = map(string)
  default = {}
}

# terraform.tfvars
additional_tags = {
  "MyTerraformSkillLevel" = "Uberhigh"
}

# main.tf
locals {
  ...
  tags = merge({
      "costCenter" = "BrightCubesInternal"
      "owner"      = var.yourname
      "region"     = var.location
    }, var.additional_tags
  )
}
```

</details>

**Bonus points**: Add another default tag that writes the current timestamp - formatted as `YYYY-MM-DD_hour:minute` - to a tag called `last_updated`. This way every time you run `terraform apply` this tag gets updated.

**Super duper extra bonus points**: Use the `lifecycle` meta-argument to ignore later changes to this tag on your virtual machine resource.

There may also be cases where you want to add parameters to your template that you want to have visible somewhere. An example we will explore here is the VM shutdown schedule. You may want to let the user configure this shutdown schedule, but also show it in the tags on the Overview pane of your virtual machine in the Azure Portal to make it clear this is configured.<p>

> First find the way Terraform configures the automated shutdown schedules for Azure VMs. (HINT: look for Azure DevTest). Next, create two variable definitions called `vm_shutdown_time` and `enable_vm_shutdown`. Add these variables to the shutdown schedule resource and add both variables to the tags of your Azure VM as a string in a `key=value` pair.

**Bonus points**: Create a variable validation rule to validate that the value inputted to the `vm_shutdown_time` variable is of the format Terraform expects for that specific property argument.

<details>
<summary>Solution</summary>

```hcl
# variables.tf
variable "vm_shutdown_time" {
  type = number
}

variable "enable_vm_shutdown" {
  type = bool
}

# terraform.tfvars
enable_vm_shutdown = true
vm_shutdown_time   = 2000

# main.tf
resource "azurerm_dev_test_global_vm_shutdown_schedule" "bctf-vm-shutdown" {
  virtual_machine_id = azurerm_linux_virtual_machine.bctf-vm.name
  location           = var.location
  enabled            = var.enable_vm_shutdown

  daily_recurrence_time = var.vm_shutdown_time
  timezone              = "W. Europe Standard Time"

  notification_settings {
    enabled = false
  }
}

resource "azurerm_linux_virtual_machine" "bctf-vm" {
  ...
  tags = merge(
    local.tags,
    { "enable_vm_shutdown" = tostring(var.enable_vm_shutdown) },
    { "vm_shutdown_time" = tostring(var.vm_shutdown_time) }
  )
  ...
```

</details>

Go to your virtual machine in the Azure Portal and verify that your tags contain your custom `additional_tag`, the values you have set for the variables and that your shutdown schedule is correctly configured under 'Auto-shutdown' in the left navigation pane.<p>

### **Dynamic expressions**

As you've noticed, some resources allow you to conditionally enable their creation through a property argument (`enabled = true`). However, this is not the case for most Terraform resources. They way you normally deal with conditional resources is through the `count` or `for_each` arguments. Read more about them [here](https://www.terraform.io/language/meta-arguments/count). While this may sometimes feel illogical, you can use this argument to create anywhere between *0* and *n* instances of a resource, and in this case `count = 0` basically tells Terraform to not create any instance of the resource. Think of `count` as a simple looping mechanism, while `for_each` gives you a few more options.

Let's go ahead and try this out. In the previous module we've created some network security group rules that allow us access to the virtual machine we've created from our own IP address. Let's say we want to conditionally enable an IP address rule that provides access to a range of IP addresses associated to our office's VPN. The user of our Terraform template can enable a boolean to enable this IP whitelisting rule. You can use the syntax for conditional expressions for this to enable conditional creation of our resource. More info is [here](https://developer.hashicorp.com/terraform/language/expressions/conditionals) and [here](https://developer.hashicorp.com/terraform/tutorials/configuration-language/expressions#use-a-conditional-expression).

> Create a variable that allows us to configure this. Next, create another `azurerm_network_security_rule` that allows access to IP ranges `"145.109.0.0/17", "145.18.0.0/16", "146.50.0.0/16"` and is conditionally created based on this variable.

<details>
<summary>Solution</summary>

```hcl
# variables.tf
variable "allowOfficeVPN" {
  type    = bool
  default = false
}

# terraform.tfvars
allowOfficeVPN = true

# main.tf
resource "azurerm_network_security_rule" "vre-allow_office_vpn" {
  count                       = var.allowOfficeVPN ? 1 : 0
  name                        = "Allow-Office_VPN"
  priority                    = 800
  direction                   = "Inbound"
  access                      = "Allow"
  protocol                    = "Tcp"
  source_port_range           = "*"
  destination_port_range      = "22"
  source_address_prefixes     = ["145.109.0.0/17", "145.18.0.0/16", "146.50.0.0/16"]
  destination_address_prefix  = "*"
  resource_group_name         = azurerm_resource_group.bctf-rg.name
  network_security_group_name = azurerm_network_security_group.bctf-nsg.name
}
```

</details>

<br>
Let's make this a bit more complex. Imagine we want to give the user of our template the option to conditionally attach a data disk to our VM to increase storage space. In this example we do not worry about attaching the disk in the OS itself, just adding it within Azure.
<br>

> First create a boolean variable definition that allows the user to specify if they want to attach a data disk. Use a size of 128GB and type `Standard_LRS`. Next, find the way that Terraform allows you to first create and then attach data disks to a virtual machine (using two separate resources). Use the `count` argument to conditionally create `1` or `0` instances of both resources by evaluating the value passed to your newly created boolean.

**Bonus points**: Create an **implicit** dependency from your new data disk on the Azure VM resource through its `name` property, by using your VM's name in the name of your disk.

<details>
<summary>Solution</summary>

```hcl
# variables.tf
variable "add_data_disk" {
  type = bool  
}

# terraform.tfvars
add_data_disk = true

# main.tf
resource "azurerm_managed_disk" "bctf-vm-datadisk" {
  count                = var.add_data_disk ? 1 : 0
  name                 = "${azurerm_linux_virtual_machine.bctf-vm.name}-datadisk"
  location             = var.location
  resource_group_name  = azurerm_resource_group.bctf-rg.name
  storage_account_type = "Standard_LRS"
  disk_size_gb         = 128
  create_option        = "Empty"
}

resource "azurerm_virtual_machine_data_disk_attachment" "bctf-vm-datadisk-attach" {
  count              = var.add_data_disk ? 1 : 0
  managed_disk_id    = azurerm_managed_disk.bctf-vm-datadisk[count.index].id
  virtual_machine_id = azurerm_linux_virtual_machine.bctf-vm.id
  lun                = "10"
  caching            = "ReadWrite"
}
```

</details>

Go to your virtual machine in the Azure Portal and verify that your disk has been added successfully 'Disks' in the left navigation pane.<p>
As you can see, you can use this `count` property to dynamically create a resource. This gives you a lot of possibilities. Another example that Terraform presents [here](https://learn.hashicorp.com/tutorials/terraform/expressions?in=terraform/configuration-language#create-a-conditional-count-criteria) is that by declaring a variable called `var.high_availability` you can dynamically create 1 or 3 instances of a VM or workload based on the way you set your variables. Or you can use a `var.osType` variable when you have both a `azurerm_linux_virtual_machine` and a `azurerm_windows_virtual_machine` in your template to dynamically create 1 or 0 instances of each.<p>
But what if you want to add more options for configuring these disks, for example by specifying the size or storage account type for any number of disks? You would have to create multiple variables for a fixed amount of disks, like `var.disk1sku` and `var.disk2size`. This is where `for_each` can help you specify more complex types without duplicating too much code.

> First, create a complex variable that contains a map of objects containing the disk name, size and type. Add two different data disks (just identify them by `1` and `2`) of different names and sizes through your `.tfvars` file by passing them as a map to this object. Finally adjust the two resources you created in the previous exercise in `main.tf` to reflect your changes using the `for_each` argument instead of `count`.<p>
> You will run into issues with your LUN. This needs to be identical for each disk. There are multiple ways to fix this so it's up to you! <p>
<details>
<summary>Hint for the LUN part</summary>
You can solve this by adding them to your `map` or by using a random ID or by using the `1` or `2` map identifiers through `each.key`.
</details>

<details>
<summary>Solution</summary>

```hcl
# variables.tf
variable "data_disks" {
  description = "Allows you to specify a map of configurations for multiple data disks. To not create any data disks, use 'data_disks={}' in your tfvars."
  
  type = map(object({
    name = string
    size = number
  }))
}

# terraform.tfvars
data_disks = {
    1 = {
      name = "smalldatadisk"
      size = 128
    },
    2 = {
      name = "bigdatadisk"
      size = 256
    }
}

# main.tf
resource "azurerm_managed_disk" "bctf-vm-datadisk" {
  for_each = var.data_disks
  
  name                 = each.value.name
  location             = var.location
  resource_group_name  = azurerm_resource_group.bctf-rg.name
  storage_account_type = "Standard_LRS"
  disk_size_gb         = each.value.size
  create_option        = "Empty"
}

resource "azurerm_virtual_machine_data_disk_attachment" "bctf-vm-datadisk-attach" {
  for_each = var.data_disks
  
  managed_disk_id    = azurerm_managed_disk.bctf-vm-datadisk[each.key].id
  virtual_machine_id = azurerm_linux_virtual_machine.bctf-vm.id
  lun                = 10 + each.key
  caching            = "ReadWrite"
}
```

</details>

There are many more functions and arguments to experiment with, but this should give you a sneak peek into what is possible with Terraform.

## Level 2: Work with modules in your Terraform configuration

As mentioned before, modules are reusable pieces of Terraform code. In the [Terraform Registry](https://registry.terraform.io/browse/modules?provider=azurerm) there are a huge amount of modules available for use in your Terraform code. You can use these, but also use any Git-based or path-based reference to a folder containing Terraform code. In the next exercises, we will work with public (Git) modules, and later turn our own Terraform configuration into a module and reference to it based on its path.

When you start working with Terraform, you will often find that you can use modules to re-use parts of your Terraform templates. If you want to compare these modules to principles in software engineering, they are mostly similar to **functions**. Instead of having to copy a piece of your code across your program, you can use *modules* to re-use parts of your code across your infrastructure configuration.

Say you want to replicate the Terraform code you wrote to deploy a virtual machine to multiple environments, you want to prevent having to copy your code to a folder called *dev* and a folder called *prod*. By using modules, you can define a top-level template and re-use it for a dev and a prod configuration. For a more extensive explanation, [check out this great post by Gruntwork](https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d).

Technically every Terraform configuration you write can be turned into a module. Your variables become the module inputs, and your outputs become the module outputs. The values that you assign using your `terraform.tfvars` file are now the inputs for your new module. The outputs that you defined in your `outputs.tf` file can now be used in your Terraform configuration.

This may sound confusing, so let's practice with some examples.

### Using local modules

We can get started with a very simple example: a naming convention module. As mentioned before, a module can be as little as a template that takes **module inputs** through its **variables**, and can generate **module outputs** using its **outputs**. Any set of Terraform configuration files in a folder (as you've been working now) can be used as a module.

> Create some simple Terraform configuration in a new folder called `name_module` in the folder where you are have been writing your code up until now. This Terraform code takes two variables you currently use in your naming convention: `yourname` and `location`. You don't have to define any `data sources` or `resources`. What it outputs is the value you currently use as `local.rootname`. Give the output the name `nameconv`. TIP: Try out Terraform's `join` function for concatenating your values.

<details>
<summary>Solution</summary>

```bash
## Folder structure
# New module
-modules
--name_module
---main.tf # can also be split up in multiple .tf files
# Code up until now
main.tf
outputs.tf
terraform.tfvars
variables.tf
```

```hcl
# main.tf
variable "yourname" {}
variable "location" {}

output "nameconv" {
  value = join("-",[
    "bctf",
    var.yourname,
    var.location])
} 
# same value as you had before with 
# local.rootname = "bctf-${var.yourname}-${var.location}"
```

</details>

To use this **re-usable module** (child) inside your **root module** (parent), where you've been working in until now, use the following syntax:

```hcl
module "<NAME>" {
  source = "<SOURCE>"

  [CONFIG ...]
}
```

Here `<NAME>` is the same identifier you have used up until now to identify `resources`, `<SOURCE>` is the path where the module code can be found (in this case `name_module`), and `CONFIG` consists of your values for the variables for your template. For example, you can adjust your `main.tf` and use the `name_module` in it as follows:

```hcl
# main.tf
module "naming_convention" {
  source = "./modules/name_module"

  location = "westeurope"
  yourname = "tdejong"
}
```

You can now use the module output, called `nameconv`, like this: `module.<NAME>.<OUTPUT_NAME>`. So, for our module, that becomes: `module.naming_convention.nameconv`. Go ahead and made the adjustment to your code and replace every instance of `local.rootname` with this, and run `terraform apply`.

You will see that Terraform has detected a new module. Whether this module is local, from the Terraform registry or from a Git repository, Terraform will always have to initialize the module, which basically means that it will download a local copy of the module. Run `terraform init` now and see that it has initialized your module together with your configured providers:

<details>
<summary>Output</summary>

```hcl
Initializing modules...
- naming_convention in name_module

Initializing the backend...

Initializing provider plugins...
- Reusing previous version of hashicorp/azurerm from the dependency lock file
- Reusing previous version of hashicorp/tls from the dependency lock file
- Using previously-installed hashicorp/azurerm v3.43.0
- Using previously-installed hashicorp/tls v4.0.4

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
```

</details>

Check out the `modules.json` file at `.terraform\modules\modules.json` in your working directory to see how Terraform is behaving under the hood. Run `terraform apply` again to run your changes. **Nothing should change**, but our configuration now uses the `name_module` to generate a naming convention for your resources. Now, when you that developing new modules or new Terraform configs, you can use this module to have the same naming convention everywhere without duplicating any code in `local` blocks between different configurations.

Now let's continue by making this a bit more complex, and using modules that are publicly available.

### Using public modules

The Terraform Registry is one place where you can find public modules, but you often also see that modules are shared between teams in a company. These are then often shared in a shared Git repository. But now we will only practice with public modules.
Let's change our configuration a bit. We are currently using an existing virtual network referencing a `data` source, but let's create our own virtual network and subnets using the official Azure VNet module that is maintained by Microsoft. **If you are using your own subscription and have already created your own VNet, you can skip to the next assignment. Or do it anyway, whatever you want :)**

> Find the Azure/vnet module in the Terraform Registry. Check out the code for the module on GitHub to see how it works, and reference the `Inputs` to check which variables are required and which are optional. Replace the virtual network and subnet resource in our `main.tf` with a `module` block. Add a second subnet. Be sure to change the `address_space` and `subnet_prefixes`. Assign the network security group we created earlier to both subnets. Also add our tags block. Make sure you update the existing references to the virtual network and subnet to reflect the `output` values of the module.

<details>
<summary>Solution</summary>

```hcl
module "azure-vnet" {
  source = "Azure/vnet/azurerm" # simple syntax for Terraform Registry modules

  vnet_name           = "${module.naming_convention.nameconv}-vnet"
  resource_group_name = azurerm_resource_group.bctf-rg.name
  address_space       = ["10.1.0.0/16"]
  subnet_prefixes     = ["10.1.90.0/24", "10.1.100.0/24"]
  subnet_names        = ["subnet1", "subnet2"]
  use_for_each        = true
  vnet_location       = var.location

  subnet_service_endpoints = {
    subnet1 = ["Microsoft.Storage", "Microsoft.Sql", "Microsoft.KeyVault"]
    subnet2 = ["Microsoft.KeyVault"]
  }

  nsg_ids = {
    subnet1 = azurerm_network_security_group.bctf-nsg.id
    subnet2 = azurerm_network_security_group.bctf-nsg.id
  }

  tags = local.tags
}

resource "azurerm_network_interface" "bctf-nic" {
...
  ip_configuration {
    name                          = "${module.naming_convention.nameconv}-nic-cfg"
    subnet_id                     = module.azure-vnet.vnet_subnets[0]
...

resource "azurerm_key_vault" "bctf-kv" {
...
  network_acls {
    default_action             = "Deny"
    bypass                     = "AzureServices"
    virtual_network_subnet_ids = module.azure-vnet.vnet_subnets
    ip_rules                   = ["${var.my_ip_address}"] # Your own IP address
  }
...
```

</details>

**NOTE:** You will probably run into some issues with your NIC and the virtual machine not being able to handle the removal of your existing subnet. For the sake of this workshop you can choose to keep them to prevent the errors, or you can use `terraform taint` to force recreation of the NIC and the VM when you run `terraform apply` next. Or try out a `terraform destroy` to see how quickly you can rebuild your infrastructure!

### Alternative exercise

Find any example public module for the `azurerm` provider in the [Terraform registry](https://registry.terraform.io/browse/modules?provider=azurerm) and apply it to your configuration.

</details>

## Level 3: Store state in a remote backend

By default, Terraform stores state locally in a file named terraform.tfstate. When working with Terraform in a team, use of a local file makes Terraform usage complicated because each user must make sure they always have the latest state data before running Terraform and make sure that nobody else runs Terraform at the same time, since Terraform locks the file during its operations.

With remote state, Terraform writes the state data to a remote data store, which can then be shared between all members of a team. Terraform supports storing state in Terraform Cloud, HashiCorp Consul, Amazon S3, Azure Blob Storage, Google Cloud Storage, Alibaba Cloud OSS, and more. The place where the state is stored by Terraform is called a `backend`.

By default, Terraform uses a backend called local, which stores state as a local file on disk. But in any use case where you are not the only person using the Terraform infrastructure, you should always configure a Terraform backend. In this module, we will configure an `azurerm` backend and store our Terraform state in an Azure storage account. Since the state file contains sensitive data, you should make sure that the storage account you use for Terraform state files is well protected.

> If you're using the shared subscription you can use the pre-created storage account and storage container. You can also create one manually. Or, for **BONUS POINTS**, create one yourself by finding a module in the Terraform Registry that allows you to create a storage account and `tfstates` container. Once you have it set up, continue with the next assignment:

> Read about the `azurerm` backend [here](https://developer.hashicorp.com/terraform/language/settings/backends/azurerm). Create a file `backend.tf` containing an `azurerm` backend configuration. The storage account name is `bcworkshoptfstates` in resource group `bctf-workshop-rg` and the container name is `tfstates`. You can think of your own `key` (filename) and you can just use your Azure CLI authentication mechanism.

<details>
<summary>Solution</summary>

```hcl
# backend.tf
terraform {
  backend "azurerm" {
    resource_group_name  = "bctf-workshop-rg"
    storage_account_name = "bcworkshoptfstates"
    container_name       = "tfstates"
    key                  = "tommy.terraform.tfstate"
  }
}
```
</details>

Now, when you run `terraform apply` again to initialize the new backend configuration, Terraform will output that it will need to initialize the new backend:

<details>
<summary>Output</summary>

```hcl
Error: Backend initialization required, please run "terraform init"
│
│ Reason: Initial configuration of the requested backend "azurerm"
│
│ The "backend" is the interface that Terraform uses to store state,
│ perform operations, etc. If this message is showing up, it means that the
│ Terraform configuration you're using is using a custom configuration for
│ the Terraform backend.
│
│ Changes to backend configurations require reinitialization. This allows
│ Terraform to set up the new configuration, copy existing state, etc. Please run
│ "terraform init" with either the "-reconfigure" or "-migrate-state" flags to
│ use the current configuration.
│
│ If the change reason above is incorrect, please verify your configuration
│ hasn't changed and try again. At this point, no changes to your existing
│ configuration or state have been made.
```

</details>

Initialize the new backend configuration by running `terraform init` now.<p>
You will be asked if you want to copy the existing state file. Enter "yes" since we do want to migrate from a local to a remote backend! Check the contents of your local `terraform.tfstate` file and verify that the file is now empty, since we've moved this state file to the storage account.

In most CI/CD scenarios you will use service principals or Azure CLI to authenticate to an `azurerm` backend. However, you can also specify any of the lines in the backend configuration as a command-line parameter or through a separate file, after with you can pass them to the `terraform init` command using `-backend-config=PATH` (for files) or `-backend-config="KEY=VALUE"` for inline variables, such as `-backend-config="access_key=$SUPERSECRETKEY_IN_MY_DEVOPS_RUNNER"`.

We are now one step closer to moving our code away from our local computer and preparing it for re-use. Let's move on to the next part and start preparing our code for turning it into a module!

## Level 4: Turn your code into a reusable module

Up to this point you've already created a pretty nice block of Terraform configuration. Now is the point where you turn all your work into a module so that you can distribute it to multiple environments or your colleagues. As you've seen in previous exercises, there is not that much needed to make your code reusable: a module is already functional if it contains your `main.tf`, `outputs.tf` and `variables.tf`. Any code that is **specific** to your current configuration should be left out of your module:
  
- **Provider configuration**: A provider is configured in your root module, and is shared between modules to run a combination of modules and your own code. Each module can however declare its own provider requirements, so that Terraform can ensure that there is a single version of the provider that is compatible with all modules in the configuration. More info [here](https://developer.hashicorp.com/terraform/language/modules/develop/providers) and [here](https://developer.hashicorp.com/terraform/language/providers/requirements).
- **Backend configuration**: The backend is specific to a run of Terraform, and is never part of a reusable module.
- **Variables declaration (*.tfvars)**: The way you define variables for your module is within the `module` block where you call your module and supply the module inputs. You can however still configure your root module with variables, but you will have to created a new `variables.tf` for your root module. The `variables.tf` you've created before is part of your reusable module!
- **Terraform-generated files**: It may go without saying, but anything in the `.terraform` folder or a copy of your state file (`*.tfstate`) should never be included in your module folder. They may contain sensitive data and are generally specific to your current Terraform configuration.

> Given your current folder structure with all Terraform files in the root folder, and the `name_module` in a separate folder, move your reusable module code to its own directory called `bctf_vm_module`.  NOTE: Be sure to also fix the path reference to the `name_module` to reference the new folder structure.  

<details>
<summary>Solution</summary>

```bash
## Folder structure
# New module
-modules
--name_module
---main.tf # can also be split up in multiple .tf files
--bctf_vm_module
---main.tf
---outputs.tf
---variables.tf
# Remaining files
backend.tf
terraform.tfvars
```

</details>

Congratulations, you have created your first module! It's easy as that. <p>
The next step is to set your module input variables, which are basically just the variables that you've set before using your `*.tfvars` file. You can simply copy the key-value pairs to the module definition block.

> Create a new `main.tf` file in the root folder that also contains your backend configuration. Here, be sure to configure your providers and call your module the same way you learned with the `name_module` in level 2.

<details>
<summary>Solution</summary>

 ```hcl
# main.tf
provider "azurerm" {
  features {}
}

provider "tls" {}

module "bctf_vm" {
  source = "./modules/bctf_vm_module"

  location      = "westeurope"
  yourname      = "tdejong"
  my_ip_address = "<your_ip_address>"
  additional_tags = {
    "MyTerraformSkillLevel" = "Uberhigh"
  }
  allowOfficeVPN = true
  enable_vm_shutdown = true
  vm_shutdown_time = 2000
  data_disks = {
      1 = {
        name = "smalldatadisk"
        size = 128
      },
      2 = {
        name = "bigdatadisk"
        size = 256
      }
  }
}
```

</details>

Now, when you run `terraform apply`, Terraform will detect that you've moved your resources to a child module, which changes it's resource ID. Because of this, Terraform will prompt that it will delete all the current resources and replace them. To prevent this, you must let Terraform know that you intend to move resources rather than replace them.

```bash
Plan: 25 to add, 0 to change, 25 to destroy.
```

Generally, this is not an issue as you can just recreate your resources, but there can always be cases where you do not want this behavior. To prevent this, you can use the `moved` configuration block to track your resource moves in the configuration itself. Read more about this block [here](https://developer.hashicorp.com/terraform/tutorials/modules/move-config).

> Let's say we want to keep our SSH key but that it's fine that all our resources will be recreated to accomodate the move of our resources from a parent (root) to a child (reusable) module. Create a `moved` block to refactor your configuration and update the resource ID for your `tls_private_key` resource.

<details>
<summary>Solution</summary>

 ```hcl
# main.tf
moved {
  from = tls_private_key.bctf-ssh-key
  to   = module.bctf_vm.tls_private_key.bctf-ssh-key
}
```

</details>

Your output should now be:

```bash
Plan: 24 to add, 0 to change, 24 to destroy.
```

**Bonus points:** you can further separate parts of your current module into a networking module, disk module, compute module, however far you want to go. Terraform writes about this here: [Module Creation - Recommended Pattern](https://developer.hashicorp.com/terraform/tutorials/certification-associate-tutorials-003/pattern-module-creation).

## Level 5: Next steps

You've made it to the end of the workshop! Over the past three modules you have worked with most of Terraform's features and have experienced how powerful it can be. There are a few more steps to take from here if you want to dive deeper into the tool, but those are all for bonus points ;)

- **Environments**: You can dive deeper into how Terraform suggests to deal with separating configuration per environment. Your module is already a great start, since you can now use the same infrastructure across different environments. You could simply add an `env` variable and add it to your naming convention. But there is also the concept of Terraform workspaces which allow you to separate state files per workspace. Read more about the possibilities in this great blog post series [here](https://blog.gruntwork.io/how-to-manage-multiple-environments-with-terraform-32c7bc5d692).
- **Publishing your module**: If you want to share your module with others, you can upload it to a Git repository. The `source` argument you used to reference a local path in your module config can just as well be a Git URL, complete with a ref structure so you can even specify a specific tag/version of the Git repository to use in your config.
- **Work with awesome open-source extensions**: Next to the huge amount of community providers and modules, there are tons of powerful tools to improve your code. Just some examples:
  - [`terraform-docs`](https://terraform-docs.io/) to automatically generate documentation on input variables and outputs and many more for your Terraform modules.
  - [`tfsec`](https://aquasecurity.github.io/tfsec/v1.28.1/) for static code analysis.
  - [`checkov`](https://www.checkov.io/) for scanning security best practices.
  - [`tflint`](https://github.com/terraform-linters/tflint) for linting your Terraform code.
  - Or any of the other cool tools included in the [pre-commit hooks for Terraform](https://github.com/antonbabenko/pre-commit-terraform).

### Get certified!

As mentioned before, you've touched upon most of the subjects and tools needed to get started with Terraform. Some subjects were however left out of scope of this workshop and I advise you to read about them before doing the Terraform Associate exam. You will not get a lot of difficult questions about these subjects, but they're easy points once you've read about them once.

You can check out the full list of subjects you should familiarize with for the exam [in the Exam Review here](https://developer.hashicorp.com/terraform/tutorials/certification-003/associate-review-003). This list is very useful as it also included links to documentation and tutorials corresponding to the specific subject. There are however some subjects that were not discussed in the workshop:

- 4a Describe when to use `terraform import` to import existing infrastructure into your Terraform state
- 4c Describe when to enable verbose logging and what the outcome/value is
- 7e Manage resource drift and Terraform state
- 8b Describe secure secret injection best practice using Hashicorp Vault
- 8g Describe built-in dependency management (order of execution based)
- 9: Anything related to **Terraform Cloud**, the paid enterprise offering by HashiCorp

I can very much recommend the practice questions for the Terraform Associate exam over at [ExamTopics](https://www.examtopics.com/exams/hashicorp/terraform-associate/). Good luck!
